https://m.blog.naver.com/wideeyed/221021710286

Softmax(소프트맥스)는 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수

분류하고 싶은 클래수의 수 만큼 출력으로 구성

결국 가장 큰 값은 이미 소프트맥스 이전에 가장 큰 값이였다.

따라서 추론(운영)단계에서 연산속도를 빠르기하기 위해 생략하기도 한다.



소프트맥스 결과값을 One hot encoder의 입력으로 연결하면

가장 큰 값만 True값, 나머지는 False값이 나오게 하여 이용 가능
소프트맥스는 활성함수 중 하나로 다중 카테고리 분류 문제에 쓰인다. Softmax와 Sigmoid 모두 모델의 출력값을 0과 1사이의 값으로 뱉기 때문에 우리는 이걸 확률값으로 해석할 수 있다.
Sigmoid는 맞냐/아니냐를 뜻하는 이진 분류 문제에 사용된다. 1이면 맞고 0이면 아니다. Softmax는 k개의 클래스에 해당하는 확률값을 뱉는다. k1에 0.1 k2에 0.2 k3에 0.7, 이런 식이다.
Softmax로 계산한 확률값들의 총합은 항상 1이다(당연히). Softmax/Sigmoid의 예측 확률값과 실제값을 Cross Entropy 함수에 먹여서 Loss를 계산할 수 있다.


https://velog.io/@francomoon7/%EC%98%88%EC%B8%A1%EC%97%90-Softmax%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%A9%B4-%EC%95%88%EB%90%98%EB%8A%94-%EC%9D%B4%EC%9C%A0

크로스 엔트로피란
엔트로피란 정보 이론에서 불확실성의 척도다. 소프트맥스나 시그모이드를 사용하여 모델의 출력값을 확률값으로 바꿔주면 크로스 엔트로피를 이용하여 Loss를 계산할 수 있다.
이 말은 모델이 학습 할 수 있는 환경(경사)를 만든다 뜻이다. 이 설명만으로 소프트맥스의 역할에 대해 충분하지만, 소프트맥스를 왜 추론에 쓰지 않는 이유는 크로스 엔트로피에 달려 있다.

통계에서 불확실성(Uncertainty)이란 어떤 사건 P가 얼마나 예측하기 힘든지를 말한다.
엔트로피(Entropy)는 그 불확실성을 계산하기 위한 방법이다.
상자 안에 빨간 공과 검은 공이 5:5로 있는 경우와, 2:8로 있는 경우의 엔트로피를 계산 말고 생각해보자.
5:5의 경우에는 무엇이 나올지 예측하기가 어렵다. 2:8의 경우에는 아마 8에 해당하는 검은 공이 나올 것으로 예측할 수 있다.
이렇게 예측하기 쉬운 일에서 엔트로피는 낮아진다.
